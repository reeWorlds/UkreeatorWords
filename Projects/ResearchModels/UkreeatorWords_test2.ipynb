{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_QQnjqs3RBzr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readWords(path):\n",
        "  words = set()\n",
        "  try:\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "      words.update(set(f.read().split(\"\\n\")))\n",
        "  except:\n",
        "    with open(path, 'r', encoding=\"cp1251\") as f:\n",
        "      words.update(set(f.read().split(\"\\n\")))\n",
        "  words.discard(\"\")\n",
        "  return words\n",
        "\n",
        "def getAllWords(listNames):\n",
        "  allWords = set()\n",
        "\n",
        "  for name in listNames:\n",
        "    allWords.update(readWords(\"data/\" + name + \".txt\"))\n",
        "\n",
        "  return list(allWords)\n",
        "\n",
        "words_s = getAllWords([\"locations\", \"locations2\", \"names2\", \"names3\", \"words1\"])\n",
        "print(f\"small words list size = {len(words_s)}\")"
      ],
      "metadata": {
        "id": "jQnKMBquSElZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_set = words_s\n",
        "\n",
        "chars = set()\n",
        "for word in words_set:\n",
        "  chars.update(set(word))\n",
        "chars = sorted(list(chars))\n",
        "\n",
        "print(\"Number of unique chars = \" + str(len(chars)))\n",
        "print(\"\".join(chars))\n",
        "\n",
        "ctoi = dict([(x, i) for i, x in enumerate(chars)])\n",
        "itoc = dict([(i, x) for i, x in enumerate(chars)])\n",
        "delim = len(chars)\n",
        "size = delim + 1\n",
        "ctoi['.'] = delim\n",
        "itoc[delim] = '.'"
      ],
      "metadata": {
        "id": "7RWPjRUbSJqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log for different models:\n",
        "\n",
        "model4_tiny (p = 1,375): train_llh = 2.384 valid_llh = 2.384\n",
        "\n",
        "model4_normal (p = 11,265): train_llh = 2.166 valid_llh = 2.171\n",
        "\n",
        "model4_large (p = 23,405): train_llh = 2.078 valid_llh = 2.091\n",
        "\n",
        "model4_largest (p = 134,349): train_llh = 1.930 valid_llh = 1.965"
      ],
      "metadata": {
        "id": "r3RNY35hfDbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "RbUgughvSNlh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4_input_len = 11\n",
        "\n",
        "def m4_align_word(word):\n",
        "  len_diff = (m4_input_len - 1) - len(word)\n",
        "  if len_diff < 0:\n",
        "    word = word[-m4_input_len + 1:]\n",
        "  if len_diff > 0:\n",
        "    word = (\".\" * len_diff) + word\n",
        "  word = word + \".\"\n",
        "  return word\n",
        "\n",
        "def m4_split_word_into_tokens(word):\n",
        "  res = []\n",
        "  for i in range(len(word)):\n",
        "    res.append((m4_align_word(word[0:i]), word[i]))\n",
        "  res.append((m4_align_word(word), \".\"))\n",
        "  return res\n",
        "\n",
        "def m4_str_to_int(samples):\n",
        "  return [([ctoi[c] for c in sample[0]], ctoi[sample[1]]) for sample in samples]\n",
        "\n",
        "for x,y in m4_split_word_into_tokens(words_set[0]):\n",
        "  print(f\"{x} -> {y}\")\n",
        "\n",
        "m4_all_samples = []\n",
        "for word in words_set:\n",
        "  m4_all_samples.extend(m4_str_to_int(m4_split_word_into_tokens(word)))\n",
        "random.shuffle(m4_all_samples)\n",
        "\n",
        "print(f\"total of {len(m4_all_samples)} tokens\")\n",
        "\n",
        "m4_n_split = int(len(m4_all_samples) * 0.9)\n",
        "\n",
        "m4_x_trn = torch.tensor([sample[0] for sample in m4_all_samples[0:m4_n_split]])\n",
        "m4_y_trn = torch.tensor([sample[1] for sample in m4_all_samples[0:m4_n_split]])\n",
        "\n",
        "m4_x_vld = torch.tensor([sample[0] for sample in m4_all_samples[m4_n_split:]])\n",
        "m4_y_vld = torch.tensor([sample[1] for sample in m4_all_samples[m4_n_split:]])\n",
        "\n",
        "print(f\"train shape x = {m4_x_trn.shape} y = {m4_y_trn.shape}\")\n",
        "print(f\"validation shape x = {m4_x_vld.shape} y = {m4_y_vld.shape}\")"
      ],
      "metadata": {
        "id": "Slu--ceaSOgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4_batch_size = 32\n",
        "m4_steps_in_epoch = m4_n_split // m4_batch_size\n",
        "\n",
        "#m4_n_embd, m4_l1_channels, m4_l2_channels, m4_l3_channels, m4_l4_channels, m4_head_channels = 2, 4, 8, 8, 12, 12\n",
        "#m4_n_embd, m4_l1_channels, m4_l2_channels, m4_l3_channels, m4_l4_channels, m4_head_channels = 12, 16, 24, 24, 32, 64\n",
        "m4_n_embd, m4_l1_channels, m4_l2_channels, m4_l3_channels, m4_l4_channels, m4_head_channels = 16, 24, 32, 48, 64, 64\n",
        "#m4_n_embd, m4_l1_channels, m4_l2_channels, m4_l3_channels, m4_l4_channels, m4_head_channels = 40, 64, 96, 128, 128, 192\n",
        "\n",
        "m4_leaky_relu_alpha = 0.05"
      ],
      "metadata": {
        "id": "Z0kg9uAXaX9J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model4(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model4, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(size, m4_n_embd)\n",
        "    nn.init.kaiming_normal_(self.embedding.weight, mode='fan_in')\n",
        "\n",
        "    self.l1 = nn.Conv1d(m4_n_embd, m4_l1_channels, 2)\n",
        "    nn.init.kaiming_normal_(self.l1.weight, mode='fan_in', nonlinearity='leaky_relu', a=m4_leaky_relu_alpha)\n",
        "    self.batch1 = nn.BatchNorm1d(m4_l1_channels)\n",
        "    self.activ1 = nn.LeakyReLU(m4_leaky_relu_alpha)\n",
        "\n",
        "    self.l2 = nn.Conv1d(m4_l1_channels, m4_l2_channels, 2, stride=2)\n",
        "    nn.init.xavier_uniform_(self.l2.weight)\n",
        "    self.batch2 = nn.BatchNorm1d(m4_l2_channels)\n",
        "    self.activ2 = nn.Tanh()\n",
        "\n",
        "    self.l3 = nn.Conv1d(m4_l2_channels, m4_l3_channels, 2)\n",
        "    nn.init.xavier_uniform_(self.l3.weight)\n",
        "    self.batch3 = nn.BatchNorm1d(m4_l3_channels)\n",
        "    self.activ3 = nn.Tanh()\n",
        "\n",
        "    self.l4 = nn.Conv1d(m4_l3_channels, m4_l4_channels, 2, stride=2)\n",
        "    nn.init.xavier_uniform_(self.l4.weight)\n",
        "    self.batch4 = nn.BatchNorm1d(m4_l4_channels)\n",
        "    self.activ4 = nn.Tanh()\n",
        "\n",
        "    self.flat = nn.Flatten()\n",
        "    self.l5 = nn.Linear(2 * m4_l4_channels, m4_head_channels)\n",
        "    nn.init.xavier_uniform_(self.l5.weight)\n",
        "    self.batch5 = nn.BatchNorm1d(m4_head_channels)\n",
        "    self.activ5 = nn.Tanh()\n",
        "    \n",
        "    self.head = nn.Linear(m4_head_channels, size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x).permute(0, 2, 1)\n",
        "    x = self.activ1(self.batch1(self.l1(x)))\n",
        "    x = self.activ2(self.batch2(self.l2(x)))\n",
        "    x = self.activ3(self.batch3(self.l3(x)))\n",
        "    x = self.activ4(self.batch4(self.l4(x)))\n",
        "    x = self.activ5(self.batch5(self.l5(self.flat(x))))\n",
        "    x = self.head(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "N5Y9-uoUatmY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Model4()\n",
        "print(f\"Number of parameters in model4 = {count_parameters(model4)}\")"
      ],
      "metadata": {
        "id": "6Pg7SPK-cp48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "torch.manual_seed(74)\n",
        "m4_optimizer = torch.optim.AdamW(model4.parameters(), lr = 0.01, weight_decay = 0.001)\n",
        "m4_scheduler = torch.optim.lr_scheduler.ExponentialLR(m4_optimizer, gamma=0.5)\n",
        "\n",
        "def m4_getScore():\n",
        "  model4.eval()\n",
        "\n",
        "  t_batch = 256\n",
        "  train_llh, valid_llh = 0.0, 0.0\n",
        "  train_tot, valid_tot = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(t_batch, m4_x_vld.size(0), t_batch):\n",
        "      idx = torch.arange(i - t_batch, i, 1)\n",
        "      x, y = m4_x_vld[idx], m4_y_vld[idx]\n",
        "      \n",
        "      outputs = model4(x)\n",
        "      llh = F.cross_entropy(outputs, y)\n",
        "\n",
        "      valid_llh += llh.item() * t_batch\n",
        "      valid_tot += t_batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for i in range(t_batch, m4_x_trn.size(0), t_batch):\n",
        "      idx = torch.arange(i - t_batch, i, 1)\n",
        "      x, y = m4_x_trn[idx], m4_y_trn[idx]\n",
        "      \n",
        "      outputs = model4(x)\n",
        "      llh = F.cross_entropy(outputs, y)\n",
        "\n",
        "      train_llh += llh.item() * t_batch\n",
        "      train_tot += t_batch\n",
        "  \n",
        "  valid_llh /= valid_tot\n",
        "  train_llh /= train_tot\n",
        "\n",
        "  return train_llh, valid_llh\n",
        "\n",
        "train_llh, valid_llh = m4_getScore()\n",
        "print(f\"epoch {-1} loss = {train_llh}   {valid_llh}\")\n",
        "\n",
        "for epoch in range(10):\n",
        "  model4.train()\n",
        "\n",
        "  for step in range(m4_steps_in_epoch):\n",
        "    m4_optimizer.zero_grad()\n",
        "\n",
        "    ix = torch.randint(0, m4_x_trn.size(0), (m4_batch_size,))\n",
        "    x, y = m4_x_trn[ix], m4_y_trn[ix]\n",
        "\n",
        "    outputs = model4(x)\n",
        "    loss = F.cross_entropy(outputs, y)\n",
        "\n",
        "    loss.backward()\n",
        "    m4_optimizer.step()\n",
        "  \n",
        "  m4_scheduler.step()\n",
        "  for param_group in m4_optimizer.param_groups:\n",
        "    param_group['weight_decay'] = [param_group['lr'] for param_group in m4_optimizer.param_groups][0] * 0.1\n",
        "\n",
        "  train_llh, valid_llh = m4_getScore()\n",
        "\n",
        "  print(f\"epoch {epoch} loss = {train_llh}   {valid_llh}\")"
      ],
      "metadata": {
        "id": "RvdHfpQJdIgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'data/model4_large.pth'\n",
        "torch.save(model4.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "w5HMuqlM0Il6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "l = model4.embedding.weight.tolist()\n",
        "x_coords, y_coords = zip(*l)\n",
        "\n",
        "plt.figure(dpi=300)\n",
        "plt.scatter(x_coords, y_coords, s=400)\n",
        "\n",
        "# Add text labels to each point\n",
        "for i, (x, y) in enumerate(l):\n",
        "    plt.text(x, y, itoc[i], fontsize=12, ha='center', va='center')\n",
        "\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.title('2D Points')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wj9kd-KffrZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}